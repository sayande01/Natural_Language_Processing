{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8e22701-0288-419e-bf26-8136448ca1d2",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "**Tokenization** in NLP is the process of splitting text into smaller units called *tokens*. These tokens can be words, sentences, or characters.\n",
    "\n",
    "- **Word Tokenization**: Splitting text into words (e.g., \"I love programming\" → `[\"I\", \"love\", \"programming\"]`).\n",
    "- **Sentence Tokenization**: Splitting text into sentences (e.g., \"I love programming. It’s fun.\" → `[\"I love programming.\", \"It’s fun.\"]`).\n",
    "- **Character Tokenization**: Splitting text into individual characters (e.g., \"hello\" → `[\"h\", \"e\", \"l\", \"l\", \"o\"]`).\n",
    "- **Subword Tokenization**: Splitting words into meaningful sub-units (e.g., \"unhappiness\" → `[\"un\", \"happiness\"]`).\n",
    "\n",
    "Tokenization is essential for breaking down text into manageable parts for further NLP tasks like text classification, sentiment analysis, or machine translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d572855-3030-4e5b-baa5-58cfae44daf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\sayan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32f4761d-7228-4e33-964c-c5c769b76b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = 'Hello , My name is Sayan De. I am Data Science enthusiast.I build Machine Learning Projects focused on Predictive analytics. Right now I am trying to improve my conceptual understanding for NLP and Deep Learning.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "987f2947-95ab-4a8d-a56e-c5c918a36058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello , My name is Sayan De. I am Data Science enthusiast.I build Machine Learning Projects focused on Predictive analytics. Right now I am trying to improve my conceptual understanding for NLP and Deep Learning.\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0caf82b5-6fa7-43b5-a124-879d0abecb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "833199d4-ba4f-4a70-9ed2-498af3d0483e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sent_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "487e6ef9-fac9-4223-8644-de38e9085d63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello , My name is Sayan De.',\n",
       " 'I am Data Science enthusiast.I build Machine Learning Projects focused on Predictive analytics.',\n",
       " 'Right now I am trying to improve my conceptual understanding for NLP and Deep Learning.']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "96ea8867-5974-43c6-bf39-1e5213234f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a0e59913-c53f-46a3-b4a0-4a26e897665e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " ',',\n",
       " 'My',\n",
       " 'name',\n",
       " 'is',\n",
       " 'Sayan',\n",
       " 'De',\n",
       " '.',\n",
       " 'I',\n",
       " 'am',\n",
       " 'Data',\n",
       " 'Science',\n",
       " 'enthusiast.I',\n",
       " 'build',\n",
       " 'Machine',\n",
       " 'Learning',\n",
       " 'Projects',\n",
       " 'focused',\n",
       " 'on',\n",
       " 'Predictive',\n",
       " 'analytics',\n",
       " '.',\n",
       " 'Right',\n",
       " 'now',\n",
       " 'I',\n",
       " 'am',\n",
       " 'trying',\n",
       " 'to',\n",
       " 'improve',\n",
       " 'my',\n",
       " 'conceptual',\n",
       " 'understanding',\n",
       " 'for',\n",
       " 'NLP',\n",
       " 'and',\n",
       " 'Deep',\n",
       " 'Learning',\n",
       " '.']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5b4b54e8-2866-4149-b559-2f69531af925",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "425da992-daff-4fa4-a94d-c8fa4ee22fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c529e26a-027d-4cc7-9328-3b9acd513d67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " ',',\n",
       " 'My',\n",
       " 'name',\n",
       " 'is',\n",
       " 'Sayan',\n",
       " 'De.',\n",
       " 'I',\n",
       " 'am',\n",
       " 'Data',\n",
       " 'Science',\n",
       " 'enthusiast.I',\n",
       " 'build',\n",
       " 'Machine',\n",
       " 'Learning',\n",
       " 'Projects',\n",
       " 'focused',\n",
       " 'on',\n",
       " 'Predictive',\n",
       " 'analytics.',\n",
       " 'Right',\n",
       " 'now',\n",
       " 'I',\n",
       " 'am',\n",
       " 'trying',\n",
       " 'to',\n",
       " 'improve',\n",
       " 'my',\n",
       " 'conceptual',\n",
       " 'understanding',\n",
       " 'for',\n",
       " 'NLP',\n",
       " 'and',\n",
       " 'Deep',\n",
       " 'Learning',\n",
       " '.']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4478d0f7-8a87-4c1d-bbf1-0ab178953403",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tok.tokenize(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9136836a-6a6f-48d8-b107-117961cacb79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
